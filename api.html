

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The pyoptgra module &mdash; Pyoptgra 1.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=6efca38a"></script>
      <script src="_static/doctools.js?v=9a2dae69"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="GODOT Example" href="example-notebook.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Pyoptgra
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="sensitivity.html">Sensitivity Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="sensitivity.html#functions-for-sensitivity-analysis">Functions for Sensitivity Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="scaling-and-tolerances.html">Variable Scaling Factors</a></li>
<li class="toctree-l1"><a class="reference internal" href="scaling-and-tolerances.html#constraint-tolerances">Constraint Tolerances</a></li>
<li class="toctree-l1"><a class="reference internal" href="changes-from-fortran.html">Internal Workings of the Python to Fortran Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="example-notebook.html">GODOT Example</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">The pyoptgra module</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pyoptgra.optgra"><code class="docutils literal notranslate"><span class="pre">optgra</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pyoptgra.optgra.evolve"><code class="docutils literal notranslate"><span class="pre">optgra.evolve()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pyoptgra.optgra.prepare_sensitivity"><code class="docutils literal notranslate"><span class="pre">optgra.prepare_sensitivity()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pyoptgra.optgra.sensitivity_matrices"><code class="docutils literal notranslate"><span class="pre">optgra.sensitivity_matrices()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pyoptgra.optgra.linear_update_new_callable"><code class="docutils literal notranslate"><span class="pre">optgra.linear_update_new_callable()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#pyoptgra.optgra.linear_update_delta"><code class="docutils literal notranslate"><span class="pre">optgra.linear_update_delta()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#the-optgra-c-wrapper">The optgra C++ wrapper</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#_CPPv4N6optgra8optimizeERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbiiddNSt6vectorIdEENSt6vectorIdEENSt6vectorIiEENSt6vectorINSt6stringEEENSt6vectorINSt6stringEEEiiNSt6vectorIdEENSt6vectorIiEEii"><code class="docutils literal notranslate"><span class="pre">optimize()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#_CPPv4N6optgra28compute_sensitivity_matricesERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEENSt6vectorINSt6stringEEENSt6vectorINSt6stringEEEiNSt6vectorIdEENSt6vectorIiEEii"><code class="docutils literal notranslate"><span class="pre">compute_sensitivity_matrices()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#_CPPv4N6optgra25prepare_sensitivity_stateERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEEiNSt6vectorIdEENSt6vectorIiEEii"><code class="docutils literal notranslate"><span class="pre">prepare_sensitivity_state()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#_CPPv4N6optgra24get_sensitivity_matricesE17sensitivity_stateRKNSt6vectorIiEE6vectorIiEd"><code class="docutils literal notranslate"><span class="pre">get_sensitivity_matrices()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#_CPPv4N6optgra31sensitivity_update_new_callableE17sensitivity_stateRKNSt6vectorIiEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEEiNSt6vectorIdEEii"><code class="docutils literal notranslate"><span class="pre">sensitivity_update_new_callable()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#_CPPv4N6optgra35sensitivity_update_constraint_deltaE17sensitivity_stateRKNSt6vectorIiEERKNSt6vectorIiEER6vectorIdEddNSt6vectorIdEEii"><code class="docutils literal notranslate"><span class="pre">sensitivity_update_constraint_delta()</span></code></a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyoptgra</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">The pyoptgra module</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/api.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-pyoptgra-module">
<span id="api"></span><h1>The pyoptgra module<a class="headerlink" href="#the-pyoptgra-module" title="Link to this heading"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="pyoptgra.optgra">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyoptgra.</span></span><span class="sig-name descname"><span class="pre">optgra</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyoptgra.optgra" title="Link to this definition"></a></dt>
<dd><p>This class is a user defined algorithm (UDA) providing a wrapper around OPTGRA, which is written
in Fortran.</p>
<p>It is specifically designed for near-linear optimization problems with many constraints. When
optimizing a problem, Optgra will first move towards satisfying the constraints, then move along
the feasible region boundary to optimize the merit function, fixing constraint violations as
they occur.</p>
<p>For this, constraints and the merit function are linearized. Optgra will perform less well on
very non-linear merit functions or constraints.</p>
<p>Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">pygmo</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">pyoptgra</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prob</span> <span class="o">=</span> <span class="n">pygmo</span><span class="o">.</span><span class="n">problem</span><span class="p">(</span><span class="n">pygmo</span><span class="o">.</span><span class="n">schwefel</span><span class="p">(</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pop</span> <span class="o">=</span> <span class="n">pygmo</span><span class="o">.</span><span class="n">population</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">algo</span> <span class="o">=</span> <span class="n">pygmo</span><span class="o">.</span><span class="n">algorithm</span><span class="p">(</span><span class="n">pyoptgra</span><span class="o">.</span><span class="n">optgra</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pop</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">evolve</span><span class="p">(</span><span class="n">pop</span><span class="p">)</span>
</pre></div>
</div>
<p>Initialize a wrapper instance for the OPTGRA algorithm.</p>
<p>Some of the construction arguments, for example the scaling factors, depend on the dimension
of the problem. Passing a problem with a different dimension to the instance’s evolve
function will result in an error.</p>
<p>Some problem-specific options are deduced from the problem in the population given to the
evolve function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_iterations</strong> – maximum number of total iterations</p></li>
<li><p><strong>max_correction_iterations</strong> – number of
constraint correction iterations in the beginning
If no feasible solution is found within that many iterations, Optgra aborts</p></li>
<li><p><strong>max_distance_per_iteration</strong> – maximum scaled distance traveled in each iteration</p></li>
<li><p><strong>perturbation_for_snd_order_derivatives</strong> – Used as delta for numerically computing
second order errors of the constraints in the optimization step</p></li>
<li><p><strong>variable_scaling_factors</strong> – optional - Scaling factors for the input variables.
If passed, must be positive and as many as there are variables</p></li>
<li><p><strong>variable_types</strong> – optional - Flags to set variables to either free (0) or fixed (1). Fixed
variables are also called parameters in sensitivity analysis. If passed, must be as
many flags as there are variables</p></li>
<li><p><strong>constraint_priorities</strong> – optional - lower constraint priorities are fulfilled earlier.
During the initial constraint correction phase, only constraints with a priority at
most k are considered in iteration k. Defaults to zero, so that all constraints are
considered from the beginning.</p></li>
<li><p><strong>bounds_to_constraints</strong> – optional - if true (default), translate box bounds of the given
problems into inequality constraints for optgra. Note that when also passing
constraint priorities, the original constraints of the problem come first, followed
by those derived from the lower box bounds, then those from the upper box bounds.
Infinite bounds are ignored and not counted.</p></li>
<li><p><strong>bound_constraints_tolerance</strong> – optional - constraint tolerance for the constraints derived
from bounds</p></li>
<li><p><strong>merit_function_threshold</strong> – optional - convergence threshold for merit
function</p></li>
<li><p><strong>force_bounds</strong> – optional - whether to force the bounds given by the problem. If
false (default), the fitness function might also be called with values of x that are
outside of the bounds. Set to true if the fitness function cannot handle that.
If active, the gradients evaluated near the bounds will be inacurate potentially
leading to convergence issues.</p></li>
<li><p><strong>khan_bounds</strong> – <p>optional - whether to gracefully enforce bounds on the decision vector
using Michael Khan’s method, by default False.:</p>
<div class="math notranslate nohighlight">
\[x = \frac{x_{max} + x_{min}}{2} + \frac{x_{max} - x_{min}}{2} \cdot \sin(x_{Khan})\]</div>
<p>Where <span class="math notranslate nohighlight">\(x\)</span> is the pagmo decision vector and <span class="math notranslate nohighlight">\(x_{Khan}\)</span> is the decision
vector passed to OPTGRA. In this way parameter bounds are guaranteed to be
satisfied, but the gradients near the bounds approach zero.
Pyoptgra uses a variant of the above method that additionally scales the
argument of the <span class="math notranslate nohighlight">\(\sin\)</span> function such that the derivatives
<span class="math notranslate nohighlight">\(\frac{d x_{Khan}}{d x}\)</span> are unity in the center of the box bounds.
Alternatively, to a <span class="math notranslate nohighlight">\(\sin\)</span> function, also a <span class="math notranslate nohighlight">\(\tanh\)</span> or Fourier expansion
of a triangle wave can be used as a Khan function.
Valid input values are: True (same as ‘sin’),’sin’, ‘tanh’, ‘triangle1’,
‘triangle2’, ‘triangle3’… and False.</p>
</p></li>
<li><p><strong>optimization_method</strong> – select 0 for steepest descent, 1 for modified spectral conjugate
gradient method, 2 for spectral conjugate gradient method and 3 for conjugate
gradient method</p></li>
<li><p><strong>log_level</strong> – Control the original screen output of OPTGRA. 0 has no output,
4 and higher have maximum output`. Set this to 0 if you want to use the pygmo
logging system based on <cite>set_verbosity()</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – if optimization_method is not one of 0, 1, 2, or 3 ValueError: if any of</p></li>
<li><p><strong>max_iterations</strong><strong>, </strong><strong>max_correction_iterations</strong><strong>, </strong><strong>max_distance_per_iteration</strong><strong>,</strong> – or perturbation_for_snd_order_derivatives are negative</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pyoptgra.optgra.evolve">
<span class="sig-name descname"><span class="pre">evolve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">population</span> <span class="pre">pop</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyoptgra.optgra.evolve" title="Link to this definition"></a></dt>
<dd><p>Call OPTGRA with the best-fitness member of the population as start value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>population</strong> – The population containing the problem and a set of initial solutions.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The changed population.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ValueError</strong> – If the population is empty</p></li>
<li><p><strong>ValueError</strong> – If the problem contains multiple objectives</p></li>
<li><p><strong>ValueError</strong> – If the problem is stochastic</p></li>
<li><p><strong>ValueError</strong> – If the problem dimensions don’t fit to constraint_priorities
    or variable_scaling_factors that were passed to the wrapper constructor</p></li>
<li><p><strong>ValueError</strong> – If the problem has finite box bounds and bounds_to_constraints was
    set to True in the wrapper constructor (default), constraint_priorities
    were also passed but don’t cover the additional bound-derived constraints</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyoptgra.optgra.prepare_sensitivity">
<span class="sig-name descname"><span class="pre">prepare_sensitivity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">problem</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyoptgra.optgra.prepare_sensitivity" title="Link to this definition"></a></dt>
<dd><p>Prepare OPTGRA for sensitivity analysis at x. This is independant from previous and later
calls to evolve, but enables calls to sensitivity_matrices, linear_update_new_callable and
linear_update_delta on this instance.</p>
<p>This works by creating a linearization of the problem’s fitness function around x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>problem</strong> – The problem being analyzed. x: The value of x around which linearization is</p></li>
<li><p><strong>performed</strong></p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If the problem contains multiple objectives ValueError: If the problem is</p></li>
<li><p><strong>stochastic ValueError</strong> – If the problem dimensions don’t fit to constraint_priorities
    or variable_scaling_factors that were passed to the wrapper constructor</p></li>
<li><p><strong>ValueError</strong> – If the problem has finite box bounds and bounds_to_constraints was
    set to True in the wrapper constructor (default), constraint_priorities were also
    passed but don’t cover the additional bound-derived constraints</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyoptgra.optgra.sensitivity_matrices">
<span class="sig-name descname"><span class="pre">sensitivity_matrices</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyoptgra.optgra.sensitivity_matrices" title="Link to this definition"></a></dt>
<dd><p>Get stored sensitivity matrices prepared by earlier call to prepare_sensivitity. Note that
active constraints are constraints that are currently fulfilled, but could be violated in
the next iteration. Parameters refer to variables whose variable type was declared as fixed.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a boolean list of whether each constraint is
active, the sensitivity of constraints + merit function with respect to active
constraints, the sensitivity of constraints + merit function with respect to parameters,
the sensitivity of variables with respect to active constraints, and the sensitivity of
variables with respect to parameters.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple of one list and four matrices</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>RuntimeError</strong> – If prepare_sensitivity has not been called on this instance</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyoptgra.optgra.linear_update_new_callable">
<span class="sig-name descname"><span class="pre">linear_update_new_callable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">problem</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyoptgra.optgra.linear_update_new_callable" title="Link to this definition"></a></dt>
<dd><p>Perform a single optimization step on the stored value of x, but with a new callable</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>problem</strong> – A problem containing the new callable.
Has to have same dimensions and types as the problem passed to
prepare_sensitivity</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>tuple of new_x, new_y</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>RuntimeError</strong> – If prepare_sensitivity has not been called on this instance ValueError: If</p></li>
<li><p><strong>number</strong><strong> or </strong><strong>type</strong><strong> of </strong><strong>constraints</strong><strong> of </strong><strong>the new problem are different from</strong> – those of the problem passed to prepare_sensitivity</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pyoptgra.optgra.linear_update_delta">
<span class="sig-name descname"><span class="pre">linear_update_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constraint_delta</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pyoptgra.optgra.linear_update_delta" title="Link to this definition"></a></dt>
<dd><p>Perform a single optimization step on the linear approximation prepared with
prepare_sensitivity. For this, no new function calls to the problem callable are performed,
making this potentially very fast.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>constraint_delta</strong> – A list of deltas against the constraints. They are subtracted from the</p></li>
<li><p><strong>values.</strong> (<em>stored</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>tuple of new_x, new_y</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>RuntimeError</strong> – If prepare_sensitivity has not been called on this instance ValueError: If</p></li>
<li><p><strong>number</strong><strong> of </strong><strong>deltas does not fit number</strong><strong> of </strong><strong>constraints.</strong> – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="the-optgra-c-wrapper">
<h1>The optgra C++ wrapper<a class="headerlink" href="#the-optgra-c-wrapper" title="Link to this heading"></a></h1>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N6optgra8optimizeERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbiiddNSt6vectorIdEENSt6vectorIdEENSt6vectorIiEENSt6vectorINSt6stringEEENSt6vectorINSt6stringEEEiiNSt6vectorIdEENSt6vectorIiEEii">
<span id="_CPPv3N6optgra8optimizeERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbiiddNSt6vectorIdEENSt6vectorIdEENSt6vectorIiEENSt6vectorINSt6stringEEENSt6vectorINSt6stringEEEiiNSt6vectorIdEENSt6vectorIiEEii"></span><span id="_CPPv2N6optgra8optimizeERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbiiddNSt6vectorIdEENSt6vectorIdEENSt6vectorIiEENSt6vectorINSt6stringEEENSt6vectorINSt6stringEEEiiNSt6vectorIdEENSt6vectorIiEEii"></span><span id="optgra::optimize__std::vector:double:CR.std::vector:i:CR.fitness_callback.gradient_callback.b.i.i.double.double.std::vector:double:.std::vector:double:.std::vector:i:.std::vector:ss:.std::vector:ss:.i.i.std::vector:double:.std::vector:i:.i.i"></span><span class="target" id="wrapper_8hpp_1aa252c841c49099bb3070a6e4f353680e"></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">tuple</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">optgra</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">optimize</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">initial_x</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">constraint_types</span></span>, <span class="n"><span class="pre">fitness_callback</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">fitness</span></span>, <span class="n"><span class="pre">gradient_callback</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">gradient</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">has_gradient</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">max_iterations</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">150</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">max_correction_iterations</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">90</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">max_distance_per_iteration</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">10</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">perturbation_for_snd_order_derivatives</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">convergence_thresholds</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">variable_scaling_factors</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">constraint_priorities</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">string</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">variable_names</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">string</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">constraint_names</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">optimization_method</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">2</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">derivatives_computation</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">autodiff_deltas</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">variable_types</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">log_level</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">verbosity</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6optgra8optimizeERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbiiddNSt6vectorIdEENSt6vectorIdEENSt6vectorIiEENSt6vectorINSt6stringEEENSt6vectorINSt6stringEEEiiNSt6vectorIdEENSt6vectorIiEEii" title="Link to this definition"></a><br /></dt>
<dd><p>Main C++ wrapper function. </p>
<p>Call optgra to optimize a problem. Most of the parameters are identical to the constructor arguments of pyoptgra, but some additional ones are available.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>initial_x</strong> – the initial guess for the decision vector </p></li>
<li><p><strong>constraint_types</strong> – types of constraints. Set 0 for equality constraints, -1 for inequality constraints that should be negative, 1 for positive inequality constraints and -2 for unenforced constraints. Last element describes the merit function, with -1 for minimization problems and 1 for maximization problems. </p></li>
<li><p><strong>fitness</strong> – a callable for the fitness values. It is called with the current x, expected output is an array of first all equality constraints, then all inequality constraints, and last the merit function </p></li>
<li><p><strong>gradient</strong> – a callable for the gradient values, optional. It is called with the current x, expected output is a two-dimensional array g, with g_ij being the gradient of constraint i with respect to input variable j. </p></li>
<li><p><strong>has_gradient</strong> – whether the problem has a gradient. If set to False, the gradient callable will not be called and numerical differentiation will be used instead. </p></li>
<li><p><strong>max_iterations</strong> – the maximum number of iterations. Optional, defaults to 150. </p></li>
<li><p><strong>max_correction_iterations</strong> – number of constraint correction iterations in the beginning. If no feasible solution is found within that many iterations, Optgra aborts. Optional, defaults to 90. </p></li>
<li><p><strong>max_distance_per_iteration</strong> – maximum scaled distance traveled in each iteration. Optional, defaults to 10 </p></li>
<li><p><strong>perturbation_for_snd_order_derivatives</strong> – used as delta for numerically computing second order errors of the constraints in the optimization step. Parameter VARSND in Fortran. Optional, defaults to 1 </p></li>
<li><p><strong>convergence_thresholds</strong> – tolerance a constraint can deviate and still be considered fulfilled. Constraints with lower thresholds will be prioritized during optimization. Thresholds of 0 break the optimization process. </p></li>
<li><p><strong>variable_scaling_factors</strong> – scaling factors for the input variables. If passed, must be positive and as many as there are variables </p></li>
<li><p><strong>constraint_priorities</strong> – filter in which to consider constraints. Lower constraint priorities are fulfilled earlier. During the initial constraint correction phase, only constraints with a priority at most k are considered in iteration k. Defaults to zero, so that all constraints are considered from the beginning. </p></li>
<li><p><strong>variable_names</strong> – Not yet implemented </p></li>
<li><p><strong>constraint_names</strong> – Not yet implemented </p></li>
<li><p><strong>optimization_method</strong> – select 0 for steepest descent, 1 for modified spectral conjugate gradient method, 2 for spectral conjugate gradient method and 3 for conjugate gradient method. Parameter OPTMET in Fortran. </p></li>
<li><p><strong>derivatives_computation</strong> – method to compute gradients. 0 is no gradient, 1 is the user-defined gradient function, 2 is a numerical gradient with double differencing, 3 a numerical gradient with single differencing. Parameter VARDER in Fortran. </p></li>
<li><p><strong>autodiff_deltas</strong> – deltas used for each variable when computing the gradient numerically. Optional, defaults to 0.001. </p></li>
<li><p><strong>variable_types</strong> – Optional array, specifiying 0 (normal, default) or 1 (fixed, only used for sensitivity) for each variable. </p></li>
<li><p><strong>log_level</strong> – original OPTGRA logging output: 0 has no output, 4 and higher have maximum output. Set this to 0 if you want to use the pygmo logging system based on <code class="docutils literal notranslate"><span class="pre">set_verbosity()</span></code>. </p></li>
<li><p><strong>verbosity</strong> – pygmo-style logging output: 0 has no output, N means an output every Nth cost function evaluation. Set <code class="docutils literal notranslate"><span class="pre">log_level</span></code> to zero to use this.</p></li>
</ul>
</dd>
<dt class="field-even">Throws<span class="colon">:</span></dt>
<dd class="field-even"><p><span><span class="cpp-expr sig sig-inline cpp"><span class="n">unspecified</span></span></span> – any exception thrown by memory errors in standard containers </p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a tuple of the best value of x, the fitness of that x, and a status flag of optgra</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N6optgra28compute_sensitivity_matricesERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEENSt6vectorINSt6stringEEENSt6vectorINSt6stringEEEiNSt6vectorIdEENSt6vectorIiEEii">
<span id="_CPPv3N6optgra28compute_sensitivity_matricesERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEENSt6vectorINSt6stringEEENSt6vectorINSt6stringEEEiNSt6vectorIdEENSt6vectorIiEEii"></span><span id="_CPPv2N6optgra28compute_sensitivity_matricesERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEENSt6vectorINSt6stringEEENSt6vectorINSt6stringEEEiNSt6vectorIdEENSt6vectorIiEEii"></span><span id="optgra::compute_sensitivity_matrices__std::vector:double:CR.std::vector:i:CR.fitness_callback.gradient_callback.b.double.double.std::vector:double:.std::vector:ss:.std::vector:ss:.i.std::vector:double:.std::vector:i:.i.i"></span><span class="target" id="wrapper_8hpp_1a237325cdd78122f3978142b1ec7d4d5e"></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">tuple</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">optgra</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">compute_sensitivity_matrices</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">x</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">constraint_types</span></span>, <span class="n"><span class="pre">fitness_callback</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">fitness</span></span>, <span class="n"><span class="pre">gradient_callback</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">gradient</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">has_gradient</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">max_distance_per_iteration</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">10</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">perturbation_for_snd_order_derivatives</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">variable_scaling_factors</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">string</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">variable_names</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">string</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">constraint_names</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">derivatives_computation</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">autodiff_deltas</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">variable_types</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">log_level</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">verbosity</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6optgra28compute_sensitivity_matricesERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEENSt6vectorINSt6stringEEENSt6vectorINSt6stringEEEiNSt6vectorIdEENSt6vectorIiEEii" title="Link to this definition"></a><br /></dt>
<dd><p>Compute sensitivity state and matrices in one go, without creating a sensitivity state tuple. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – the decision vector around which the sensitivity analysis is to be performed </p></li>
<li><p><strong>constraint_types</strong> – types of constraints. Set 0 for equality constraints, -1 for inequality constraints that should be negative, 1 for positive inequality constraints and -2 for unenforced constraints Last element describes the merit function, with -1 for minimization problems and 1 for maximization problems. </p></li>
<li><p><strong>fitness</strong> – a callable for the fitness values. It is called with the current x, expected output is an array of first all equality constraints, then all inequality constraints, and last the merit function </p></li>
<li><p><strong>gradient</strong> – a callable for the gradient values, optional. It is called with the current x, expected output is a two-dimensional array g, with g_ij being the gradient of constraint i with respect to input variable j. </p></li>
<li><p><strong>has_gradient</strong> – whether the problem has a gradient. If set to False, the gradient callable will not be called and numerical differentiation will be used instead. </p></li>
<li><p><strong>max_distance_per_iteration</strong> – maximum scaled distance traveled in each iteration. Optional, defaults to 10 </p></li>
<li><p><strong>perturbation_for_snd_order_derivatives</strong> – used as delta for numerically computing second order errors of the constraints in the optimization step. Parameter VARSND in Fortran. Optional, defaults to 1 </p></li>
<li><p><strong>variable_scaling_factors</strong> – scaling factors for the input variables. If passed, must be positive and as many as there are variables </p></li>
<li><p><strong>variable_names</strong> – Not yet implemented </p></li>
<li><p><strong>constraint_names</strong> – Not yet implemented </p></li>
<li><p><strong>derivatives_computation</strong> – method to compute gradients. 0 is no gradient, 1 is the user-defined gradient function, 2 is a numerical gradient with double differencing, 3 a numerical gradient with single differencing. Parameter VARDER in Fortran. </p></li>
<li><p><strong>autodiff_deltas</strong> – deltas used for each variable when computing the gradient numerically. Optional, defaults to 0.001. </p></li>
<li><p><strong>variable_types</strong> – Optional array, specifiying 0 (normal, default) or 1 (fixed, only used for sensitivity) for each variable. </p></li>
<li><p><strong>log_level</strong> – original OPTGRA logging output: 0 has no output, 4 and higher have maximum output </p></li>
<li><p><strong>verbosity</strong> – pygmo-style logging output: 0 has no output, N means an output every Nth cost function evaluation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple of one list and four matrices: a boolean list of whether each constraint is active, the sensitivity of constraints + merit function with respect to active constraints, the sensitivity of constraints + merit function with respect to parameters, the sensitivity of variables with respect to active constraints, and the sensitivity of variables with respect to parameters. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N6optgra25prepare_sensitivity_stateERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEEiNSt6vectorIdEENSt6vectorIiEEii">
<span id="_CPPv3N6optgra25prepare_sensitivity_stateERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEEiNSt6vectorIdEENSt6vectorIiEEii"></span><span id="_CPPv2N6optgra25prepare_sensitivity_stateERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEEiNSt6vectorIdEENSt6vectorIiEEii"></span><span id="optgra::prepare_sensitivity_state__std::vector:double:CR.std::vector:i:CR.fitness_callback.gradient_callback.b.double.double.std::vector:double:.i.std::vector:double:.std::vector:i:.i.i"></span><span class="target" id="wrapper_8hpp_1a14b402297916c5e973836290fc44f52d"></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">tuple</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">sensitivity_state</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">optgra</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">prepare_sensitivity_state</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">x</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">constraint_types</span></span>, <span class="n"><span class="pre">fitness_callback</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">fitness</span></span>, <span class="n"><span class="pre">gradient_callback</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">gradient</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">has_gradient</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">max_distance_per_iteration</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">10</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">perturbation_for_snd_order_derivatives</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">variable_scaling_factors</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">derivatives_computation</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">autodiff_deltas</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">variable_types</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">log_level</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">verbosity</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6optgra25prepare_sensitivity_stateERKNSt6vectorIdEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEEiNSt6vectorIdEENSt6vectorIiEEii" title="Link to this definition"></a><br /></dt>
<dd><p>Create a state tuple usable for sensitivity updates. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – the decision vector around which the sensitivity analysis is to be performed </p></li>
<li><p><strong>constraint_types</strong> – types of constraints. Set 0 for equality constraints, -1 for inequality constraints that should be negative, 1 for positive inequality constraints and -2 for unenforced constraints Last element describes the merit function, with -1 for minimization problems and 1 for maximization problems. </p></li>
<li><p><strong>fitness</strong> – a callable for the fitness values. It is called with the current x, expected output is an array of first all equality constraints, then all inequality constraints, and last the merit function </p></li>
<li><p><strong>gradient</strong> – a callable for the gradient values, optional. It is called with the current x, expected output is a two-dimensional array g, with g_ij being the gradient of constraint i with respect to input variable j. </p></li>
<li><p><strong>has_gradient</strong> – whether the problem has a gradient. If set to False, the gradient callable will not be called and numerical differentiation will be used instead. </p></li>
<li><p><strong>max_distance_per_iteration</strong> – maximum scaled distance traveled in each iteration. Optional, defaults to 10 </p></li>
<li><p><strong>perturbation_for_snd_order_derivatives</strong> – used as delta for numerically computing second order errors of the constraints in the optimization step. Parameter VARSND in Fortran. Optional, defaults to 1 </p></li>
<li><p><strong>variable_scaling_factors</strong> – scaling factors for the input variables. If passed, must be positive and as many as there are variables </p></li>
<li><p><strong>derivatives_computation</strong> – method to compute gradients. 0 is no gradient, 1 is the user-defined gradient function, 2 is a numerical gradient with double differencing, 3 a numerical gradient with single differencing. Parameter VARDER in Fortran. </p></li>
<li><p><strong>autodiff_deltas</strong> – deltas used for each variable when computing the gradient numerically. Optional, defaults to 0.001. </p></li>
<li><p><strong>variable_types</strong> – Optional array, specifiying 0 (normal, default) or 1 (fixed, only used for sensitivity) for each variable. </p></li>
<li><p><strong>log_level</strong> – original OPTGRA logging output: 0 has no output, 4 and higher have maximum output </p></li>
<li><p><strong>verbosity</strong> – pygmo-style logging output: 0 has no output, N means an output every Nth cost function evaluation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple of the current sensitivity state and the x for which the sensitivity analysis was performed. It may be different from the x given as argument, if optimization steps were performed in the meantime. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N6optgra24get_sensitivity_matricesE17sensitivity_stateRKNSt6vectorIiEE6vectorIiEd">
<span id="_CPPv3N6optgra24get_sensitivity_matricesE17sensitivity_stateRKNSt6vectorIiEE6vectorIiEd"></span><span id="_CPPv2N6optgra24get_sensitivity_matricesE17sensitivity_stateRKNSt6vectorIiEE6vectorIiEd"></span><span id="optgra::get_sensitivity_matrices__sensitivity_state.std::vector:i:CR.vector:i:.double"></span><span class="target" id="wrapper_8hpp_1aceed8f4965a198842160154c361f9d1c"></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">tuple</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">optgra</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">get_sensitivity_matrices</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">sensitivity_state</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">state_tuple</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">variable_types</span></span>, <span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">constraint_types</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">max_distance_per_iteration</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">10</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6optgra24get_sensitivity_matricesE17sensitivity_stateRKNSt6vectorIiEE6vectorIiEd" title="Link to this definition"></a><br /></dt>
<dd><p>Compute sensitivity matrics from a sensitivity state tuple. </p>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N6optgra31sensitivity_update_new_callableE17sensitivity_stateRKNSt6vectorIiEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEEiNSt6vectorIdEEii">
<span id="_CPPv3N6optgra31sensitivity_update_new_callableE17sensitivity_stateRKNSt6vectorIiEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEEiNSt6vectorIdEEii"></span><span id="_CPPv2N6optgra31sensitivity_update_new_callableE17sensitivity_stateRKNSt6vectorIiEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEEiNSt6vectorIdEEii"></span><span id="optgra::sensitivity_update_new_callable__sensitivity_state.std::vector:i:CR.std::vector:i:CR.fitness_callback.gradient_callback.b.double.double.std::vector:double:.i.std::vector:double:.i.i"></span><span class="target" id="wrapper_8hpp_1a0a7517bb70ba273b21b4415eb627cc6c"></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">tuple</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">optgra</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">sensitivity_update_new_callable</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">sensitivity_state</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">state_tuple</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">variable_types</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">constraint_types</span></span>, <span class="n"><span class="pre">fitness_callback</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">fitness</span></span>, <span class="n"><span class="pre">gradient_callback</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">gradient</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">has_gradient</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">max_distance_per_iteration</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">10</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">perturbation_for_snd_order_derivatives</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">variable_scaling_factors</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">derivatives_computation</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">autodiff_deltas</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">log_level</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">verbosity</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6optgra31sensitivity_update_new_callableE17sensitivity_stateRKNSt6vectorIiEERKNSt6vectorIiEE16fitness_callback17gradient_callbackbddNSt6vectorIdEEiNSt6vectorIdEEii" title="Link to this definition"></a><br /></dt>
<dd><p>Perform one optimization step with a new fitness callable, starting from the value of x that was set with prepare_sensitivity_state. </p>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N6optgra35sensitivity_update_constraint_deltaE17sensitivity_stateRKNSt6vectorIiEERKNSt6vectorIiEER6vectorIdEddNSt6vectorIdEEii">
<span id="_CPPv3N6optgra35sensitivity_update_constraint_deltaE17sensitivity_stateRKNSt6vectorIiEERKNSt6vectorIiEER6vectorIdEddNSt6vectorIdEEii"></span><span id="_CPPv2N6optgra35sensitivity_update_constraint_deltaE17sensitivity_stateRKNSt6vectorIiEERKNSt6vectorIiEER6vectorIdEddNSt6vectorIdEEii"></span><span id="optgra::sensitivity_update_constraint_delta__sensitivity_state.std::vector:i:CR.std::vector:i:CR.vector:double:R.double.double.std::vector:double:.i.i"></span><span class="target" id="wrapper_8hpp_1a48305af3eef7ee90f9822ae388ff3dd3"></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">tuple</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">optgra</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">sensitivity_update_constraint_delta</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">sensitivity_state</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">state_tuple</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">variable_types</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">constraint_types</span></span>, <span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">delta</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">max_distance_per_iteration</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">10</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">perturbation_for_snd_order_derivatives</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">vector</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">double</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">variable_scaling_factors</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="p"><span class="pre">{</span></span><span class="p"><span class="pre">}</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">log_level</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1</span></span>, <span class="kt"><span class="pre">int</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">verbosity</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6optgra35sensitivity_update_constraint_deltaE17sensitivity_stateRKNSt6vectorIiEERKNSt6vectorIiEER6vectorIdEddNSt6vectorIdEEii" title="Link to this definition"></a><br /></dt>
<dd><p>Perform an update step based on the prepared sensitivity state, without any calls to the fitness callbacks. </p>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="example-notebook.html" class="btn btn-neutral float-left" title="GODOT Example" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, European Space Agency.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>